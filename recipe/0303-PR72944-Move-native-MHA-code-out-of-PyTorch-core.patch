From 473e488c10f91a95c99ae884ed357cb85c5ca06a Mon Sep 17 00:00:00 2001
From: Deepali Chourasia <deepch23@in.ibm.com>
Date: Tue, 24 May 2022 13:43:01 +0000
Subject: [PATCH] Move native MHA code out of PyTorch core

---
 aten/src/ATen/native/attention.cpp            | 237 -----------------
 aten/src/ATen/native/cuda/attention.cu        | 250 ------------------
 aten/src/ATen/native/native_functions.yaml    |   5 -
 .../check_forward_backward_compatibility.py   |   1 -
 tools/build_variables.bzl                     |   1 -
 torch/overrides.py                            |   1 -
 6 files changed, 495 deletions(-)
 delete mode 100644 aten/src/ATen/native/attention.cpp
 delete mode 100644 aten/src/ATen/native/cuda/attention.cu

diff --git a/aten/src/ATen/native/attention.cpp b/aten/src/ATen/native/attention.cpp
deleted file mode 100644
index 188de5e2cd..0000000000
--- a/aten/src/ATen/native/attention.cpp
+++ /dev/null
@@ -1,237 +0,0 @@
-#include <type_traits>
-
-#include <ATen/ATen.h>
-#include <ATen/AccumulateType.h>
-#include <ATen/Dispatch.h>
-#include <ATen/NativeFunctions.h>
-#include <ATen/Parallel.h>
-#include <ATen/cpu/vec/vec256/vec256.h>
-
-namespace at {
-
-namespace native {
-
-namespace {
-
-Tensor gemm_nt(const Tensor& a, const Tensor& b) {
-  auto a_ = a.view({a.size(0) * a.size(1), a.size(2)});
-  auto b_ = b.transpose(1, 0);
-  auto c_ = at::native::matmul(a_, b_);
-  return c_.view({a.size(0), a.size(1), b.size(0)});
-}
-
-// compute q = (q + q_bias) / sqrt(dim_per_head), k = k + k_bias, v = v + v_bias
-std::tuple<Tensor, Tensor, Tensor> transform_bias_rescale_qkv(
-    const Tensor& qkv,
-    const Tensor& qkv_bias) {
-  auto B = qkv.size(0);
-  auto T = qkv.size(1);
-  auto _3D = qkv.size(2);
-  auto D = _3D / 3;
-  auto dim_per_head = 64;
-  auto num_head = D / dim_per_head;
-  auto q_k_v = at::empty({3, B, num_head, T, dim_per_head}, qkv.options());
-
-  AT_DISPATCH_FLOATING_TYPES_AND2(
-      ScalarType::Half,
-      ScalarType::BFloat16,
-      qkv.scalar_type(),
-      "transform_bias_rescale_qkv",
-      [&] {
-        scalar_t* qkv_data = qkv.data_ptr<scalar_t>();
-        scalar_t* qkv_bias_data = qkv_bias.data_ptr<scalar_t>();
-        scalar_t* q_k_v_data = q_k_v.data_ptr<scalar_t>();
-
-        int64_t grain_size =
-            std::min(internal::GRAIN_SIZE / (3 * dim_per_head), (int64_t)1);
-        parallel_for(
-            0, B * num_head * T, grain_size, [&](int64_t begin, int64_t end) {
-              for (auto i : c10::irange(begin, end)) {
-                auto t = i % T;
-                i /= T;
-                auto nh = i % num_head;
-                i /= num_head;
-                auto b = i;
-                using Vec = vec::Vectorized<scalar_t>;
-                auto V = vec::Vectorized<scalar_t>::size();
-                // TODO: handle epilogue
-                for (auto dh = 0; dh < dim_per_head / V; dh += V) {
-                  auto d = nh * dim_per_head + dh;
-                  // load
-                  auto q_bias_data = Vec::loadu(&qkv_bias_data[d + 0 * D]);
-                  auto k_bias_data = Vec::loadu(&qkv_bias_data[d + 1 * D]);
-                  auto v_bias_data = Vec::loadu(&qkv_bias_data[d + 2 * D]);
-
-                  auto q_data =
-                      Vec::loadu(&qkv_data[b * _3D * T + t * _3D + d + 0 * D]) +
-                      q_bias_data;
-                  auto k_data =
-                      Vec::loadu(&qkv_data[b * _3D * T + t * _3D + d + 1 * D]) +
-                      k_bias_data;
-                  auto v_data =
-                      Vec::loadu(&qkv_data[b * _3D * T + t * _3D + d + 2 * D]) +
-                      v_bias_data;
-
-                  q_data = q_data / Vec(8.0);
-
-                  q_data.store(&q_k_v_data
-                                   [0 * B * num_head * T * dim_per_head +
-                                    b * num_head * T * dim_per_head +
-                                    num_head * T * dim_per_head +
-                                    t * dim_per_head + dh]);
-                  k_data.store(&q_k_v_data
-                                   [1 * B * num_head * T * dim_per_head +
-                                    b * num_head * T * dim_per_head +
-                                    num_head * T * dim_per_head +
-                                    t * dim_per_head + dh]);
-                  v_data.store(&q_k_v_data
-                                   [2 * B * num_head * T * dim_per_head +
-                                    b * num_head * T * dim_per_head +
-                                    num_head * T * dim_per_head +
-                                    t * dim_per_head + dh]);
-                }
-              }
-            });
-      });
-  auto q_k_v_s =
-      at::native::split(q_k_v.view({3 * B, num_head, T, dim_per_head}), B, 0);
-  return std::make_tuple(q_k_v_s[0], q_k_v_s[1], q_k_v_s[2]);
-}
-
-Tensor bmm_nt(const Tensor& a, const Tensor& b) {
-  auto a_ = a.view({a.size(0) * a.size(1), a.size(2), a.size(3)});
-  auto b_ = b.view({b.size(0) * b.size(1), b.size(2), b.size(3)});
-  auto bt_ = b_.transpose(2, 1);
-  // TODO: are these a single call to cublas batched matmul?
-  auto c_ = at::matmul(a_, bt_);
-  return c_.view({a.size(0), a.size(1), a.size(2), b.size(2)});
-}
-
-void masked_softmax_dropout(
-    const Tensor& attn_scores,
-    const c10::optional<Tensor>& attn_mask) {
-  auto B = attn_scores.size(0);
-  auto num_heads = attn_scores.size(1);
-  auto T = attn_scores.size(2);
-  if (attn_mask) {
-    TORCH_CHECK(attn_mask->is_contiguous());
-  }
-  AT_DISPATCH_FLOATING_TYPES_AND2(
-      ScalarType::Half,
-      ScalarType::BFloat16,
-      attn_scores.scalar_type(),
-      "masked_softmax_dropout",
-      [&] {
-        using accscalar_t = acc_type<scalar_t, false>;
-        // TODO: proper implementation with masking.
-        scalar_t* attn_scores_data = attn_scores.data_ptr<scalar_t>();
-        int64_t grain_size = std::min(internal::GRAIN_SIZE / T, (int64_t)1);
-        parallel_for(
-            0, B * num_heads * T, grain_size, [&](int64_t begin, int64_t end) {
-              for (const auto i : c10::irange(begin, end)) {
-                using Vec = vec::Vectorized<scalar_t>;
-                auto V = vec::Vectorized<scalar_t>::size();
-
-                scalar_t* input_data = attn_scores_data + i * T;
-                auto max_input = Vec(std::numeric_limits<scalar_t>::lowest());
-                // TODO: handle epilogue
-                for (auto t = 0; t < T; t += V) {
-                  auto v = Vec::loadu(&input_data[t]);
-                  max_input = vec::maximum(max_input, v);
-                }
-
-                auto hmax = std::numeric_limits<scalar_t>::lowest();
-                for (auto i = 0; i < V; ++i) {
-                  hmax = std::max(max_input[i], hmax);
-                }
-                accscalar_t hsum = 0;
-                for (auto t = 0; t < T; t += V) {
-                  auto v = Vec::loadu(&input_data[t]);
-                  // TODO: vectorize in accscalar_t?
-                  for (auto i = 0; i < V; ++i) {
-                    hsum += std::exp(static_cast<accscalar_t>(v[i]) - hmax);
-                  }
-                }
-                auto inv_denominator = 1.0 / hsum;
-                for (auto t = 0; t < T; t += V) {
-                  Vec v = Vec::loadu(&input_data[t]);
-
-                  // TODO: vectorize in accscalar_t?
-                  // TODO this faster solution does not work on Android build
-                  /*
-                  for (auto i = 0; i < V; ++i) {
-                    v[i] = static_cast<scalar_t>(std::exp(static_cast<accscalar_t>(v[i]) - hmax) * inv_denominator);
-                  }
-                  v.store(&input_data[t]);
-                  */
-                  for (auto i = 0; i < V; ++i) {
-                    input_data[t + i] = static_cast<scalar_t>(std::exp(static_cast<accscalar_t>(v[i]) - hmax) * inv_denominator);
-                  }
-                }
-              }
-            });
-      });
-}
-
-Tensor bmm_nn(const Tensor& a, const Tensor& b) {
-  auto a_ = a.view({a.size(0) * a.size(1), a.size(2), a.size(3)});
-  auto b_ = b.view({b.size(0) * b.size(1), b.size(2), b.size(3)});
-  // TODO: are these a single call to cublas batched matmul?
-  auto c_ = at::matmul(a_, b_);
-  return c_.view({a.size(0), a.size(1), a.size(2), b.size(3)});
-}
-
-Tensor transform_0213(const Tensor& a) {
-  // TODO: check perf vs dedicated kernel.
-  return a.permute({0, 2, 1, 3})
-      .contiguous()
-      .view({a.size(0), a.size(2), a.size(1) * a.size(3)});
-}
-
-Tensor gemm_nt_bias(const Tensor& a, const Tensor& b, const Tensor& c) {
-  auto a_ = a.view({a.size(0) * a.size(1), a.size(2)});
-  auto r_ = at::native::linear(a_, b, c);
-  return r_.view({a.size(0), a.size(1), r_.size(1)});
-}
-
-} // namespace
-
-Tensor multi_head_self_attention_cpu(
-    const Tensor& query,
-    const Tensor& qkv_weight,
-    const Tensor& qkv_bias,
-    const Tensor& proj_weight,
-    const Tensor& proj_bias,
-    const c10::optional<Tensor>& mask) {
-  // query shape: [B, T, D]
-  // qkv_weight shape: [3 * D, D]
-
-  // shape: [B, T, 3 x D]
-  auto qkv = gemm_nt(query, qkv_weight);
-
-  // shape: 3 x [B, num_head, T, dim_per_head]
-  auto q_k_v = transform_bias_rescale_qkv(qkv, qkv_bias);
-  auto q = std::get<0>(q_k_v);
-  auto k = std::get<1>(q_k_v);
-  auto v = std::get<2>(q_k_v);
-
-  // shape: [B, num_head, T, T]
-  auto qkt = bmm_nt(q, k);
-
-  // shape: [B, num_head, T, T]
-  masked_softmax_dropout(qkt, mask);
-
-  // shape: [B, num_head, T, dim_per_head]
-  auto attn_ctx = bmm_nn(qkt, v);
-
-  // shape: [B, T, D]
-  auto attn = transform_0213(attn_ctx);
-
-  // shape: [B, T, D]
-  auto proj = gemm_nt_bias(attn, proj_weight, proj_bias);
-
-  return proj;
-}
-
-} // namespace native
-} // namespace at
diff --git a/aten/src/ATen/native/cuda/attention.cu b/aten/src/ATen/native/cuda/attention.cu
deleted file mode 100644
index 798fdf300b..0000000000
--- a/aten/src/ATen/native/cuda/attention.cu
+++ /dev/null
@@ -1,250 +0,0 @@
-#include <type_traits>
-
-#include <ATen/ATen.h>
-#include <ATen/AccumulateType.h>
-#include <ATen/Dispatch.h>
-#include <ATen/NativeFunctions.h>
-#include <ATen/TensorAccessor.h>
-
-#include <ATen/cuda/CUDAContext.h>
-#include <ATen/cuda/detail/KernelUtils.h>
-#include <ATen/cuda/detail/IndexUtils.cuh>
-#include <ATen/native/cuda/Loops.cuh>
-#include <ATen/native/cuda/MemoryAccess.cuh>
-#include <ATen/native/cuda/block_reduce.cuh>
-#include <ATen/native/cuda/PersistentSoftmax.cuh>
-
-#include <c10/cuda/CUDAMathCompat.h>
-
-namespace at {
-
-namespace native {
-
-namespace {
-
-Tensor gemm_nt(const Tensor& a, const Tensor& b) {
-  auto a_ = a.view({a.size(0) * a.size(1), a.size(2)});
-  auto b_ = b.transpose(1, 0);
-  auto c_ = at::native::matmul(a_, b_);
-  return c_.view({a.size(0), a.size(1), b.size(0)});
-}
-
-template <typename scalar_t, typename accscalar_t>
-__global__ void transform_bias_rescale_qkv_kernel(
-    // [B, T, 3 * D]
-    const PackedTensorAccessor64<scalar_t, 3, RestrictPtrTraits> qkv,
-    // [3 * D]
-    const PackedTensorAccessor64<scalar_t, 1, RestrictPtrTraits> qkv_bias,
-    // [3, B, NH, T, DH]
-    PackedTensorAccessor64<scalar_t, 5, RestrictPtrTraits> q_k_v) {
-  // warp per DH.
-  // so launch B * NH * T warps.
-  auto NH = q_k_v.size(2);
-  auto T = q_k_v.size(3);
-  auto DH = q_k_v.size(4);
-
-  auto t = blockIdx.x % T;
-  auto b = blockIdx.x / T;
-
-  auto D = NH * DH;
-  constexpr int VEC = 4;
-  using LoadT = memory::aligned_vector<scalar_t, VEC>;
-
-  // FIXME: assert ((D % VEC) == 0)
-
-  for (int32_t d_v = threadIdx.x; d_v < D / VEC; d_v += blockDim.x) {
-    auto d = d_v * VEC;
-    auto nh = d / DH;
-    auto dh = d % DH;
-    scalar_t qkv_bias_q[VEC];
-    scalar_t qkv_bias_k[VEC];
-    scalar_t qkv_bias_v[VEC];
-    scalar_t qkv_q[VEC];
-    scalar_t qkv_k[VEC];
-    scalar_t qkv_v[VEC];
-
-    *reinterpret_cast<LoadT*>(&qkv_bias_q) =
-        *reinterpret_cast<const LoadT*>(&qkv_bias[d + 0 * D]);
-    *reinterpret_cast<LoadT*>(&qkv_bias_k) =
-        *reinterpret_cast<const LoadT*>(&qkv_bias[d + 1 * D]);
-    *reinterpret_cast<LoadT*>(&qkv_bias_v) =
-        *reinterpret_cast<const LoadT*>(&qkv_bias[d + 2 * D]);
-
-    *reinterpret_cast<LoadT*>(&qkv_q) =
-        *reinterpret_cast<const LoadT*>(&qkv[b][t][d + 0 * D]);
-    *reinterpret_cast<LoadT*>(&qkv_k) =
-        *reinterpret_cast<const LoadT*>(&qkv[b][t][d + 1 * D]);
-    *reinterpret_cast<LoadT*>(&qkv_v) =
-        *reinterpret_cast<const LoadT*>(&qkv[b][t][d + 2 * D]);
-
-#pragma unroll
-    // TODO: specialize for float2half2/half2float2?
-    for (auto ii = 0; ii < VEC; ++ii) {
-      qkv_q[ii] = static_cast<scalar_t>(
-          (static_cast<accscalar_t>(qkv_q[ii]) +
-           static_cast<accscalar_t>(qkv_bias_q[ii])) /
-          static_cast<accscalar_t>(8));
-      qkv_k[ii] = static_cast<scalar_t>(
-          (static_cast<accscalar_t>(qkv_k[ii]) +
-           static_cast<accscalar_t>(qkv_bias_k[ii])));
-      qkv_v[ii] = static_cast<scalar_t>(
-          (static_cast<accscalar_t>(qkv_v[ii]) +
-           static_cast<accscalar_t>(qkv_bias_v[ii])));
-    }
-    *reinterpret_cast<LoadT*>(&q_k_v[0][b][nh][t][dh]) =
-        *reinterpret_cast<const LoadT*>(&qkv_q);
-    *reinterpret_cast<LoadT*>(&q_k_v[1][b][nh][t][dh]) =
-        *reinterpret_cast<const LoadT*>(&qkv_k);
-    *reinterpret_cast<LoadT*>(&q_k_v[2][b][nh][t][dh]) =
-        *reinterpret_cast<const LoadT*>(&qkv_v);
-  }
-}
-
-// compute q = (q + q_bias) / sqrt(dim_per_head), k = k + k_bias, v = v + v_bias
-std::tuple<Tensor, Tensor, Tensor> transform_bias_rescale_qkv(
-    const Tensor& qkv,
-    const Tensor& qkv_bias) {
-  auto B = qkv.size(0);
-  auto T = qkv.size(1);
-  auto _3D = qkv.size(2);
-  auto D = _3D / 3;
-  auto dim_per_head = 64;
-  auto num_head = D / dim_per_head;
-  auto q_k_v = at::empty({3, B, num_head, T, dim_per_head}, qkv.options());
-  AT_DISPATCH_FLOATING_TYPES_AND2(
-      ScalarType::Half,
-      ScalarType::BFloat16,
-      qkv.scalar_type(),
-      "transform_bias_rescale_qkv",
-      [&] {
-        using accscalar_t = acc_type<scalar_t, true>;
-        auto threads = std::min<int32_t>(1024, D / 4);
-        auto blocks = B * T;
-        transform_bias_rescale_qkv_kernel<scalar_t, accscalar_t>
-            <<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(
-                qkv.packed_accessor64<scalar_t, 3, RestrictPtrTraits>(),
-                qkv_bias.packed_accessor64<scalar_t, 1, RestrictPtrTraits>(),
-                q_k_v.packed_accessor64<scalar_t, 5, RestrictPtrTraits>());
-        C10_CUDA_KERNEL_LAUNCH_CHECK();
-      });
-  auto q_k_v_s =
-      at::native::split(q_k_v.view({3 * B, num_head, T, dim_per_head}), B, 0);
-  return std::make_tuple(q_k_v_s[0], q_k_v_s[1], q_k_v_s[2]);
-}
-
-Tensor bmm_nt(const Tensor& a, const Tensor& b) {
-  auto a_ = a.view({a.size(0) * a.size(1), a.size(2), a.size(3)});
-  auto b_ = b.view({b.size(0) * b.size(1), b.size(2), b.size(3)});
-  auto bt_ = b_.transpose(2, 1);
-  // TODO: are these a single call to cublas batched matmul?
-  auto c_ = at::matmul(a_, bt_);
-  return c_.view({a.size(0), a.size(1), a.size(2), b.size(2)});
-}
-
-template <typename T>
-__inline__ __device__ T WarpReduceMax(T val) {
-#pragma unroll
-  for (int offset = (C10_WARP_SIZE >> 1); offset > 0; offset >>= 1) {
-    val = std::max(val, WARP_SHFL_DOWN(val, offset));
-  }
-  return val;
-}
-
-template <typename T>
-__inline__ __device__ T WarpReduceSum(T val) {
-#pragma unroll
-  for (int offset = (C10_WARP_SIZE >> 1); offset > 0; offset >>= 1) {
-    val += WARP_SHFL_DOWN(val, offset);
-  }
-  return val;
-}
-
-void masked_softmax_dropout(
-    const Tensor& attn_scores,
-    const c10::optional<Tensor>& attn_mask) {
-  auto B = attn_scores.size(0);
-  auto num_heads = attn_scores.size(1);
-  auto T = attn_scores.size(2);
-  if (attn_mask) {
-    TORCH_CHECK(attn_mask->is_contiguous());
-  }
-  AT_DISPATCH_FLOATING_TYPES_AND2(
-      ScalarType::Half,
-      ScalarType::BFloat16,
-      attn_scores.scalar_type(),
-      "masked_softmax_dropout",
-      [&] {
-        using accscalar_t = acc_type<scalar_t, true>;
-        // TODO: proper implementation with masking.
-        dispatch_softmax_forward<scalar_t, scalar_t, accscalar_t, false, false>(
-          attn_scores.data_ptr<scalar_t>(),
-          attn_scores.data_ptr<scalar_t>(),
-          T,
-          T,
-          B * num_heads * T
-        );
-      });
-}
-
-Tensor bmm_nn(const Tensor& a, const Tensor& b) {
-  auto a_ = a.view({a.size(0) * a.size(1), a.size(2), a.size(3)});
-  auto b_ = b.view({b.size(0) * b.size(1), b.size(2), b.size(3)});
-  // TODO: are these a single call to cublas batched matmul?
-  auto c_ = at::matmul(a_, b_);
-  return c_.view({a.size(0), a.size(1), a.size(2), b.size(3)});
-}
-
-Tensor transform_0213(const Tensor& a) {
-  // TODO: check perf vs dedicated kernel.
-  return a.permute({0, 2, 1, 3})
-      .contiguous()
-      .view({a.size(0), a.size(2), a.size(1) * a.size(3)});
-}
-
-Tensor gemm_nt_bias(const Tensor& a, const Tensor& b, const Tensor& c) {
-  auto a_ = a.view({a.size(0) * a.size(1), a.size(2)});
-  auto r_ = at::native::linear(a_, b, c);
-  return r_.view({a.size(0), a.size(1), r_.size(1)});
-}
-
-} // namespace
-
-Tensor multi_head_self_attention_cuda(
-    const Tensor& query,
-    const Tensor& qkv_weight,
-    const Tensor& qkv_bias,
-    const Tensor& proj_weight,
-    const Tensor& proj_bias,
-    const c10::optional<Tensor>& mask) {
-  // query shape: [B, T, D]
-  // qkv_weight shape: [3 * D, D]
-
-  // shape: [B, T, 3 x D]
-  auto qkv = gemm_nt(query, qkv_weight);
-
-  // shape: 3 x [B, num_head, T, dim_per_head]
-  auto q_k_v = transform_bias_rescale_qkv(qkv, qkv_bias);
-  auto q = std::get<0>(q_k_v);
-  auto k = std::get<1>(q_k_v);
-  auto v = std::get<2>(q_k_v);
-
-  // shape: [B, num_head, T, T]
-  auto qkt = bmm_nt(q, k);
-
-  // shape: [B, num_head, T, T]
-  masked_softmax_dropout(qkt, mask);
-
-  // shape: [B, num_head, T, dim_per_head]
-  auto attn_ctx = bmm_nn(qkt, v);
-
-  // shape: [B, T, D]
-  auto attn = transform_0213(attn_ctx);
-
-  // shape: [B, T, D]
-  auto proj = gemm_nt_bias(attn, proj_weight, proj_bias);
-
-  return proj;
-}
-
-} // namespace native
-} // namespace at
diff --git a/aten/src/ATen/native/native_functions.yaml b/aten/src/ATen/native/native_functions.yaml
index 450d10fd86..1e77a0e843 100644
--- a/aten/src/ATen/native/native_functions.yaml
+++ b/aten/src/ATen/native/native_functions.yaml
@@ -2542,11 +2542,6 @@
     CUDA: layer_norm_cuda
     CompositeImplicitAutograd: math_native_layer_norm
 
-- func: _native_multi_head_self_attention(Tensor query, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None) -> Tensor
-  dispatch:
-    CPU: multi_head_self_attention_cpu
-    CUDA: multi_head_self_attention_cuda
-
 - func: native_layer_norm_backward(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
   dispatch:
     CPU: layer_norm_backward_cpu
diff --git a/test/forward_backward_compatibility/check_forward_backward_compatibility.py b/test/forward_backward_compatibility/check_forward_backward_compatibility.py
index 88bffae167..a0b109356d 100644
--- a/test/forward_backward_compatibility/check_forward_backward_compatibility.py
+++ b/test/forward_backward_compatibility/check_forward_backward_compatibility.py
@@ -109,7 +109,6 @@ ALLOW_LIST = [
     ("aten::nanquantile", datetime.date(2022, 9, 30)),
     ("aten::_convolution_double_backward", datetime.date(2022, 3, 31)),
     ("aten::_scatter_reduce", datetime.date(2022, 1, 31)),
-    ("aten::native_multi_head_self_attention", datetime.date(9999, 1, 1)),
     ("aten::_scatter_reduce.two", datetime.date(9999, 1, 1)),
 ]
 
diff --git a/tools/build_variables.bzl b/tools/build_variables.bzl
index f78b7cb049..d43909110e 100644
--- a/tools/build_variables.bzl
+++ b/tools/build_variables.bzl
@@ -1163,7 +1163,6 @@ aten_native_source_non_codegen_list = [
     "aten/src/ATen/native/quantized/library.cpp",
     "aten/src/ATen/quantized/QTensorImpl.cpp",
     "aten/src/ATen/quantized/Quantizer.cpp",
-    "aten/src/ATen/native/attention.cpp",
     "aten/src/ATen/native/Activation.cpp",
     "aten/src/ATen/native/AdaptiveAveragePooling.cpp",
     "aten/src/ATen/native/AdaptiveAveragePooling3d.cpp",
diff --git a/torch/overrides.py b/torch/overrides.py
index 967398e08d..d42dd40ef2 100644
--- a/torch/overrides.py
+++ b/torch/overrides.py
@@ -669,7 +669,6 @@ def get_testing_overrides() -> Dict[Callable, Callable]:
         torch.native_batch_norm: lambda input, weight, bias, running_mean, running_var, training, momentum, eps: -1,
         torch.native_dropout: lambda input, p, train: -1,
         torch.native_layer_norm: lambda input, normalized_shape, weight=None, bias=None, eps=1e-05: -1,
-        torch._native_multi_head_self_attention: lambda query, qkv_weight, qkv_bias, proj_weight, proj_bias, mask=None: -1,
         torch.native_group_norm: lambda input, weight, bias, N, C, HxW, group, eps: -1,
         torch.native_norm: lambda input, p=2: -1,
         torch.native_norm: lambda input, p=2: -1,
-- 
2.34.1

