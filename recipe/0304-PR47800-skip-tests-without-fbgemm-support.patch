diff --git a/test/quantization/test_quantize.py b/test/quantization/test_quantize.py
index e54eb33..05598fe 100644
--- a/test/quantization/test_quantize.py
+++ b/test/quantization/test_quantize.py
@@ -534,6 +534,7 @@ class TestPostTrainingStatic(QuantizationTestCase):
         self.checkQuantizedLinear(model.fc)
 
 
+    @skipIfNoFBGEMM
     def test_quantized_embedding_bag(self):
         r""" Test the post-training quantization flow, serialization and scripting
         of embedding_bag modules
diff --git a/test/quantization/test_quantize_jit.py b/test/quantization/test_quantize_jit.py
index f0cd203..0b62efa 100644
--- a/test/quantization/test_quantize_jit.py
+++ b/test/quantization/test_quantize_jit.py
@@ -3032,6 +3032,7 @@ class TestQuantizeDynamicJitOps(QuantizationTestCase):
                 FunctionalLinear(weight, bias), x,
                 "quantized::linear_dynamic", tracing=tracing, dynamic=True)
 
+    @skipIfNoFBGEMM
     def test_embedding_bag(self):
         class M(torch.nn.Module):
             def __init__(self, weights):
diff --git a/test/quantization/test_quantized_module.py b/test/quantization/test_quantized_module.py
index fe0d52a..098dd8c 100644
--- a/test/quantization/test_quantized_module.py
+++ b/test/quantization/test_quantized_module.py
@@ -712,6 +712,7 @@ class TestStaticQuantizedModule(QuantizationTestCase):
         embedding_dim=st.integers(5, 50).filter(lambda x: x % 4 == 0),
         set_qconfig=st.booleans(),
     )
+    @skipIfNoFBGEMM
     def test_embedding_api(self, num_embeddings, embedding_dim, set_qconfig):
         num_lengths = np.random.randint(1, 6)
         lengths = np.random.randint(0, 21, size=num_lengths).astype(np.int32)
diff --git a/test/quantization/test_quantized_op.py b/test/quantization/test_quantized_op.py
index 7efc863..51704e3 100644
--- a/test/quantization/test_quantized_op.py
+++ b/test/quantization/test_quantized_op.py
@@ -3015,6 +3015,7 @@ class TestQuantizedEmbeddingOps(TestCase):
     """ Tests the correctness of the quantized embedding lookup operator """
     @given(num_embeddings=st.integers(10, 100),
            embedding_dim=st.integers(5, 50).filter(lambda x: x % 4 == 0))
+    @skipIfNoFBGEMM
     def test_embedding_byte(self, num_embeddings, embedding_dim):
         quant_op = torch.ops.quantized.embedding_byte
         prepack_op = torch.ops.quantized.embedding_bag_prepack
